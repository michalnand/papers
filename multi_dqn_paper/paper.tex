\documentclass[10pt,a4paper]{article}
\renewcommand{\baselinestretch}{1.0}
\usepackage{cite}
\usepackage[dvips]{graphicx}
\usepackage{psfrag}
\usepackage{color}
\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\usepackage[font=footnotesize, captionskip=10pt]{subfig}
\usepackage{tikz}
\usepackage{flushend}
\usepackage{times}
\usepackage[margin=1.5cm]{geometry}
\usepackage[slovak, english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{multirow}
\usepackage{colortbl}

\pagestyle{empty}

\hyphenation{net-works}
\newtheorem{remark}{Remark}

\begin{document}

\title{Playing multiple games using deep reinforcement learning}
\author{Michal Chovanec, Ondrej Å uch}
\date{}
\maketitle
\thispagestyle{empty}

{\bf Abstract}
In this paper we presents deep Q network (DQN) playing multiple games with the same weights - six ATARI games.
We trained network by randomly changing games after several interations, using deep reinforcement learning.
For comparison, we also trained common DQN. Instead of using 8x8 or 4x4 kernels as in (cite Atari DQN and Doom paper)
we used only 3x3 kernels, and much lower of feature maps in layers. This network is much more parameter efficient.
As results we present training score progress and network feature maps usability for each game.


\section{Introduction}


\begin{figure}[htb!]
\centering
\includegraphics[width=0.5\textwidth]{diagrams/multi_dqn.png}
\caption{Multi DQN can play six games with the same weights}

\end{figure}

\section{Experiment setup}

{\bf Network input}:
Last 4 frames, width = 48, height = 48.


{\bf Network architecture}:
Following state of the art (cite VGG Net, ResNet), we used only 3x3 kernels, instead of (cite Atari DQN, Doom, with 8x8 kernels). As activation
function the ELU is choosen, with $\alpha = 0.1$.
After four convolution and pooling layers, the full connected layer with 256 neurons is used.
On the last layer the linear activation is used. We also tried network without 256 FC layer - to reduce amount of parameters, but the training was much slower.

Proposed network architecture is : \\
$IN48\times 48\times 12 - C3\times 3\times 32 - P2\times 2 - C3\times 3\times 32 - P2\times 2 - C3\times 3\times 32 - P2\times 2 - C3\times 3\times 32 - P2\times 2 - FC256 - FC_{actions}$

Hyperparmaters of network were choosen as :
\begin{itemize}
    \item learning rate $\eta = 0.001$
    \item L1 and L2 regularization $\lambda _1 = \lambda _2 = 0.0000001$
    \item dropout $0.02$ (only after FC256 layer)
    \item minibatch size 32 for single DQN, 128 for multi DQN
    \item gradient clip $\langle -10, 10 \rangle$
\end{itemize}

\newpage
\section{Results}

First we presents training score progress. The games rewards has been chosen to
be impossible achieve positive score only by random playing. On figures \ref{img:score_result} we
can see typical RL training progress - first is agent playing randomly, taking negative rewards.
After few hundred games the agent finds good strategy and is able to achieve more and more positive
rewards.

As second part, we focused on learned features visualisation. We processed the maximum kernel response, for
all networks and all kernels. The inputs for maximum kernel response are presented on figure \ref{img:max_response}.
Those images are obtained by maximizing kernel response using gradient backpropagation, starting with random input (cite). We
visualised only convolutional layers.

To compare trained networks for learning similar features we computed correlations between networks (TODO more specify how).
Considering networks features correlation (Table \ref{tab:networks_correlation}) we assume that
multi\_net\_0 is learning different features. But we can see strongest correlation with most complicated games - Pacman and Snake,
and the lowest correlation with the simplest game - Pong \footnote{most/less complicated game - in meaning of input complexity}.

This results leads us to visualise activation heatmap - to understand what are different networks focusing on.

\begin{figure}[htb!]
    \centering
    \subfloat[Arkanoid]{\includegraphics[width=5cm]{results/progress_training_arkanoid.png}}\hfil
    \subfloat[Enduro]{\includegraphics[width=5cm]{results/progress_training_enduro.png}}\hfil
    \subfloat[Invaders]{\includegraphics[width=5cm]{results/progress_training_invaders.png}}

    \subfloat[Pacman]{\includegraphics[width=5cm]{results/progress_training_pacman.png}}\hfil
    \subfloat[Pong]{\includegraphics[width=5cm]{results/progress_training_pong.png}}\hfil
    \subfloat[Snake]{\includegraphics[width=5cm]{results/progress_training_snake.png}}
    \caption{Training score result for first 1000 games}\label{img:score_result}
\end{figure}


\begin{figure}[htb!]
\centering
\includegraphics[width=0.7\textwidth]{results/activity/kernel_features_desc.png}
\caption{Kernel maximum response inputs visualisation for different networks}\label{img:max_response}
\end{figure}


\iffalse

\begin{figure}[htb!]
    \centering
    \subfloat[Single DQN max activity]{\includegraphics[width=6cm]{results/activity/single_net_max_desc.png}}\hfil
    \subfloat[Multi DQN max activity]{\includegraphics[width=6cm]{results/activity/multi_net0_max_desc.png}}\hfil

    \subfloat[Single DQN average activity]{\includegraphics[width=6cm]{results/activity/single_net_average_desc.png}}\hfil
    \subfloat[Multi DQN average activity]{\includegraphics[width=6cm]{results/activity/multi_net0_average_desc.png}}\hfil

    \caption{Feature maps activity}\label{img:activity}
\end{figure}
\fi



\begin{table}[htb!]
\begin{tabular}{|l|c|c|c|c|c|c|c|l|}
\hline
\textbf{network}       & \textbf{arkanoid} & \textbf{enduro} & \textbf{invaders} & \textbf{pacman} & \textbf{pong} & \textbf{snake} & \textbf{multi\_net\_0} & \textbf{summary} \\ \hline
\textbf{arkanoid}      & 1.0               & 0.152           & 0.184             & 0.177           & 0.14          & 0.172          & 0.171                  & \textbf{1.9951}  \\ \hline
\textbf{enduro}        & 0.18              & 1.0             & 0.241             & 0.252           & 0.103         & 0.169          & 0.186                  & \textbf{2.1308}  \\ \hline
\textbf{invaders}      & 0.189             & 0.205           & 1.0               & 0.203           & 0.112         & 0.164          & 0.16                   & \textbf{2.0330}  \\ \hline
\textbf{pacman}        & 0.194             & 0.232           & 0.243             & 1.0             & 0.14          & 0.17           & 0.227                  & \textbf{2.2048}  \\ \hline
\textbf{pong}          & 0.118             & 0.076           & 0.086             & 0.116           & 1.0           & 0.092          & 0.114                  & \textbf{1.6013}  \\ \hline
\textbf{snake}         & 0.223             & 0.239           & 0.208             & 0.222           & 0.154         & 1.0            & 0.268                  & \textbf{2.3137}  \\ \hline
\textbf{multi\_net\_0} & 0.166             & 0.171           & 0.155             & 0.207           & 0.127         & 0.202          & 1.0                    & \textbf{2.0283}  \\ \hline
\end{tabular}


\caption{Networks correlation}\label{tab:networks_correlation}

\end{table}



\end{document}
